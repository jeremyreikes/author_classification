{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import gensim\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "import copy\n",
    "from topic_modeling import get_topics\n",
    "import pronouncing\n",
    "import textstat\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "new_cols = ['raw_text_length', 'num_words', 'avg_word_len', 'vector_avg', 'polarity', 'subjectivity', 'dale_chall', 'rhyme_frequency', 'FleischReadingEase', 'lexicon', 'word_diversity']\n",
    "false_cols = ['starts_conj', 'ends_prep', 'has_colon', 'has_semicolon', 'has_dash', 'whom', 'has_had', 'num_ings']\n",
    "empty_cols = ['entities', 'lemmas']\n",
    "parts_of_speech = ['ADJ', 'ADV', 'ADP', 'AUX', 'CONJ', 'CCONJ', 'DET', 'EOL', 'NO_TAG', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "for col in new_cols:\n",
    "    df[col] = np.zeros(shape=(len(df), ))\n",
    "for col in empty_cols:\n",
    "    df[col] = np.empty(shape=(len(df), ))\n",
    "for col in false_cols:\n",
    "    df[col] = np.zeros(shape=(len(df),))\n",
    "for pos in parts_of_speech:\n",
    "    df[pos] = np.zeros(shape=(len(df),)).astype('uint8')\n",
    "encoder = LabelEncoder()\n",
    "df['author'] = encoder.fit_transform(df.author.values)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def rhyme_frequency(text):\n",
    "    words = list(set([i.lower() for i in text.split(' ')]))\n",
    "    num_words = len(words)\n",
    "    num_rhymes = 0\n",
    "    for i in words:\n",
    "        other = copy.copy(words)\n",
    "        other.remove(i)\n",
    "        rhymes = pronouncing.rhymes(i)\n",
    "        matches = set(other).intersection(set(rhymes))\n",
    "        num_rhymes += len(matches)/2\n",
    "    return num_rhymes/num_words\n",
    "\n",
    "def add_features(row):\n",
    "    text = row.text\n",
    "    doc = nlp(text)\n",
    "    lemmas = list()\n",
    "    entities = list()\n",
    "    for token in doc:\n",
    "        if token.text == ':':\n",
    "            row['has_colon'] = 1\n",
    "        if token.text == ';':\n",
    "            row['has_semicolon'] = 1\n",
    "        if token.text == '-':\n",
    "            row['has_dash'] = 1\n",
    "        if token.text.lower() == 'whom':\n",
    "            row['whom'] = 1\n",
    "        if token.text[-3:] == 'ing':\n",
    "            row['num_ings'] += 1\n",
    "        if token.text.lower() == 'had':\n",
    "            row['has_had'] = 1\n",
    "        pos = token.pos_\n",
    "        row[pos] += 1\n",
    "        if token.is_stop or not token.is_alpha:\n",
    "            continue\n",
    "        lemma = token.lemma_.strip().lower()\n",
    "        if lemma:\n",
    "            lemmas.append(lemma)\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    blob = TextBlob(text)\n",
    "    row['subjectivity'] = blob.sentiment.subjectivity\n",
    "    row['polarity'] = blob.sentiment.polarity\n",
    "    row['starts_conj'] = int(doc[0].pos_ == 'CONJ')\n",
    "    row['ends_prep'] = int(doc[0].pos_ == 'PREP')\n",
    "    row['entities'] = entities\n",
    "    row['lemmas'] = lemmas\n",
    "    row['raw_text_length'] = len(text)\n",
    "    row['num_words'] = len(doc)\n",
    "    row['avg_word_len'] = row.raw_text_length / row.num_words\n",
    "    row['vector_avg'] = np.mean(nlp(lemmas).vector)\n",
    "    row['num_ings'] /= row['num_words']\n",
    "    row['rhyme_frequency'] = rhyme_frequency(row['text'])\n",
    "    row['dale_chall'] = textstat.dale_chall_readability_score(row['text'])\n",
    "    row['FleischReadingEase'] = textstat.flesch_reading_ease(row['text'])\n",
    "    row['lexicon'] = textstat.lexicon_count(row['text'])\n",
    "    row['word_diversity'] = row.lexicon/row.num_words\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.apply(lambda x: add_features(x), axis=1)\n",
    "# df['FleischReadingEase'] = df['FleischReadingEase'] - df['FleischReadingEase'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics(num_topics = 4, sentences = df.text.values.tolist())\n",
    "dummies = pd.get_dummies(pd.DataFrame(topics))\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.drop(['author', 'lemmas', 'entities', 'topic'], axis=1), df.author.values, test_size=0.1, random_state=0)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(np.concatenate([X_train.text, X_val.text]))\n",
    "X_train_cv = cv.transform(X_train.text)\n",
    "X_val_cv = cv.transform(X_val.text)\n",
    "\n",
    "x_train_array = pd.DataFrame(X_train_cv.toarray())\n",
    "x_val_array = pd.DataFrame(X_val_cv.toarray())\n",
    "X_train_full = pd.concat([x_train_array, X_train], axis=1)\n",
    "X_val_full = pd.concat([x_val_array, X_val], axis=1)\n",
    "\n",
    "X_train_final = X_train_full.drop('text', axis=1)\n",
    "X_val_final = X_val_full.drop('text', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remy's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from AutoCluster import AutoKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.text\n",
    "y_train = df.author\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=0,test_size=.1)\n",
    "X_test = test_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate various\n",
    "nb = MultinomialNB()\n",
    "tk = Tokenizer(lower = True,num_words=5000)\n",
    "tfidf = TfidfVectorizer(stop_words='english',max_features=12000)\n",
    "svm = SVC(kernel='linear')\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf with svm\n",
    "X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf.transform(X_test)\n",
    "X_tfidf_val = tfidf.transform(X_val)\n",
    "svm.fit(X_tfidf_train,y_train_le)\n",
    "tfidf_train_preds =svm.predict(X_tfidf_train)\n",
    "tfidf_test_preds =svm.predict(X_tfidf_test)\n",
    "tfidf_val_preds = svm.predict(X_tfidf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text (255 characters, truncating post)\n",
    "tk.fit_on_texts(df.text)\n",
    "train_post = tk.texts_to_sequences(X_train)\n",
    "X_train_post = pad_sequences(train_pre,255,truncating='post')\n",
    "val_post = tk.texts_to_sequences(X_val)\n",
    "X_val_post = pad_sequences(val_pre,255,truncating='post')\n",
    "test_post = tk.texts_to_sequences(test_df.text)\n",
    "X_test_post = pad_sequences(test_pre,255,truncating='post')\n",
    "\n",
    "tk.fit_on_texts(X_train)\n",
    "train_pre = tk.texts_to_sequences(X_train)\n",
    "X_train_pre = pad_sequences(train_pre,255,truncating='pre')\n",
    "val_pre = tk.texts_to_sequences(X_val)\n",
    "X_val_pre = pad_sequences(val_pre,255,truncating='pre')\n",
    "test_pre = tk.texts_to_sequences(X_test)\n",
    "X_test_pre = pad_sequences(test_pre,255,truncating='pre')\n",
    "\n",
    "#reformat y\n",
    "y_train_le = encoder.fit_transform(y_train)\n",
    "y_val_le = encoder.fit_transform(y_val)\n",
    "y_train_dmy = pd.get_dummies(y_train)\n",
    "y_val_dmy = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans clustering\n",
    "train_clusters_post, val_clusters_post, test_clusters_post = AutoKMeans(X_train_post,X_val_post,X_test_post,n_clusters=30)\n",
    "train_clusters_pre, val_clusters_pre, test_clusters_pre = AutoKMeans(X_train_pre,X_val_pre,X_test_pre,n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial naive bayes\n",
    "nb.fit(X_train_post,y_train_le)\n",
    "nb_post_train_preds = nb.predict(X_train_post)\n",
    "nb_post_test_preds = nb.predict(X_test_post)\n",
    "nb_post_val_preds = nb.predict(X_val_post)\n",
    "nb.fit(X_train_pre,y_train_le)\n",
    "nb_pre_train_preds = nb.predict(X_train_pre)\n",
    "nb_pre_test_preds = nb.predict(X_test_pre)\n",
    "nb_pre_val_preds = nb.predict(X_val_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine outputs of models\n",
    "train_le =pd.DataFrame({'NB_pre':nb_pre_train_preds,'NB_post':nb_post_train_preds,'TFIDF':tfidf_train_preds,\\\n",
    "                        'Cluster_post':train_clusters_post,'Cluster_pre':train_clusters_pre})\n",
    "train_dmy = pd.DataFrame(None,index=train_le.index)\n",
    "for col in train_le.columns:\n",
    "    train_dmy = train_dmy.join(pd.get_dummies(train_le[col].astype(str),drop_first=True,prefix=col))        \n",
    "val_le =pd.DataFrame({'NB_pre':nb_pre_val_preds,'NB_post':nb_post_val_preds,'TFIDF':tfidf_val_preds,\\\n",
    "                        'Cluster_post':val_clusters_post,'Cluster_pre':val_clusters_pre})\n",
    "val_dmy = pd.DataFrame(None,index=val_le.index)\n",
    "for col in val_le.columns:\n",
    "    val_dmy = val_dmy.join(pd.get_dummies(val_le[col].astype(str),drop_first=True,prefix=col))\n",
    "test_le =pd.DataFrame({'NB_pre':nb_pre_test_preds,'NB_post':nb_post_test_preds,'TFIDF':tfidf_test_preds,\\\n",
    "                        'Cluster_post':test_clusters_post,'Cluster_pre':test_clusters_pre})\n",
    "test_dmy = pd.DataFrame(None,index=test_le.index)\n",
    "for col in test_le.columns:\n",
    "    test_dmy = test_dmy.join(pd.get_dummies(test_le[col].astype(str),drop_first=True,prefix=col))                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bagged lstm for post tokens\n",
    "num_words = 5000\n",
    "embed_vec_len = 32\n",
    "max_sequence_len = 255\n",
    "lstm_nn = models.Sequential()\n",
    "lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint('255post.best.hdf5',  verbose=1, save_best_only=True, mode='auto')\n",
    "lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepaths = []\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_train_post)),k=5000)\n",
    "    filepath = '255post'+str(counter)+'.best.hdf5'\n",
    "    filepaths.append(filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "    lstm_nn.fit(X_train_post[sub],y_train_dmy.iloc[sub,:],epochs=5,batch_size=256, validation_data=(X_val_post,y_val_dmy), callbacks=[checkpoint])\n",
    "    counter += 1\n",
    "nn_val_preds = []\n",
    "nn_train_preds = []\n",
    "nn_test_preds = []\n",
    "for filepath in tqdm.tqdm(filepaths):\n",
    "    lstm_nn = models.Sequential()\n",
    "    lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "    lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "    lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "    lstm_nn.load_weights(filepath)\n",
    "    lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    nn_train_preds.append(lstm_nn.predict(X_train_post))\n",
    "    nn_val_preds.append(lstm_nn.predict(X_val_post))\n",
    "    nn_test_preds.append(lstm_nn.predict(X_test_post))\n",
    "nn_train_post_mean = np.asarray(nn_train_preds).mean(axis=0)[:,1:]\n",
    "nn_test_post_mean = np.asarray(nn_test_preds).mean(axis=0)[:,1:]\n",
    "nn_val_post_mean = np.asarray(nn_val_preds).mean(axis=0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bagged lstm for pre tokens\n",
    "num_words = 5000\n",
    "embed_vec_len = 32\n",
    "max_sequence_len = 255\n",
    "lstm_nn = models.Sequential()\n",
    "lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepaths = []\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_train_pre)),k=5000)\n",
    "    filepath = '255pre'+str(counter)+'.best.hdf5'\n",
    "    filepaths.append(filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "    lstm_nn.fit(X_train_pre[sub],y_train_dmy.iloc[sub,:],epochs=3,batch_size=256, validation_data=(X_val_pre,y_val_dmy), callbacks=[checkpoint])\n",
    "    counter += 1\n",
    "nn_val_preds = []\n",
    "nn_train_preds = []\n",
    "nn_test_preds = []\n",
    "for filepath in tqdm.tqdm(filepaths):\n",
    "    lstm_nn = models.Sequential()\n",
    "    lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "    lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "    lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "    lstm_nn.load_weights(filepath)\n",
    "    lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    nn_train_preds.append(lstm_nn.predict(X_train_pre))\n",
    "    nn_val_preds.append(lstm_nn.predict(X_val_pre))\n",
    "    nn_test_preds.append(lstm_nn.predict(X_test_pre))\n",
    "nn_train_pre_mean = np.asarray(nn_train_preds).mean(axis=0)[:,1:]\n",
    "nn_test_pre_mean = np.asarray(nn_test_preds).mean(axis=0)[:,1:]\n",
    "nn_val_pre_mean = np.asarray(nn_val_preds).mean(axis=0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_df_train = train_dmy.join(pd.DataFrame(nn_train_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))\n",
    "nlp_df_val.csv = val_dmy.join(pd.DataFrame(nn_val_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))\n",
    "test_nlp_df = test_dmy.join(pd.DataFrame(nn_test_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp_df.to_csv('nlp_df_train.csv')\n",
    "val_nlp_df.to_csv('nlp_df_val.csv')\n",
    "test_nlp_df.to_csv('nlp_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
