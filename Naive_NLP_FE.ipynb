{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import pronouncing\n",
    "import copy\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.text\n",
    "y_train = df.author\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=0,test_size=.1)\n",
    "X_test = test_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "tk = Tokenizer(lower = True,num_words=5000)\n",
    "tfidf = TfidfVectorizer(stop_words='english',max_features=12000)\n",
    "svm = SVC(kernel='linear')\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf with svm\n",
    "# X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "# X_tfidf_test = tfidf.transform(X_test)\n",
    "# X_tfidf_val = tfidf.transform(X_val)\n",
    "# svm.fit(X_tfidf_train,y_train_le)\n",
    "# tfidf_train_preds =svm.predict(X_tfidf_train)\n",
    "tfidf_test_preds =svm.predict(X_tfidf_test)\n",
    "# tfidf_val_preds = svm.predict(X_tfidf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text (255 characters, truncating post)\n",
    "tk.fit_on_texts(df.text)\n",
    "train_post = tk.texts_to_sequences(X_train)\n",
    "X_train_post = pad_sequences(train_pre,255,truncating='post')\n",
    "val_post = tk.texts_to_sequences(X_val)\n",
    "X_val_post = pad_sequences(val_pre,255,truncating='post')\n",
    "test_post = tk.texts_to_sequences(test_df.text)\n",
    "X_test_post = pad_sequences(test_pre,255,truncating='post')\n",
    "\n",
    "tk.fit_on_texts(X_train)\n",
    "train_pre = tk.texts_to_sequences(X_train)\n",
    "X_train_pre = pad_sequences(train_pre,255,truncating='pre')\n",
    "val_pre = tk.texts_to_sequences(X_val)\n",
    "X_val_pre = pad_sequences(val_pre,255,truncating='pre')\n",
    "test_pre = tk.texts_to_sequences(X_test)\n",
    "X_test_pre = pad_sequences(test_pre,255,truncating='pre')\n",
    "\n",
    "#reformat y\n",
    "y_train_le = encoder.fit_transform(y_train)\n",
    "y_val_le = encoder.fit_transform(y_val)\n",
    "y_train_dmy = pd.get_dummies(y_train)\n",
    "y_val_dmy = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clusters with dropped columns\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "def clustering(X_train,X_val, X_test,n_clusters=15):\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(1,n_clusters)\n",
    "    for k in tqdm.tqdm(K):\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km = km.fit(X_train)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "    fig= plt.figure(figsize=(6,3))\n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    num = input('Num clusters?')\n",
    "    km = KMeans(n_clusters=int(num))\n",
    "    train_clusters = km.fit_predict(X_train)\n",
    "    val_clusters = km.fit_predict(X_val)\n",
    "    test_clusters = km.fit_predict(X_test)\n",
    "    return train_clusters, val_clusters, test_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:07<00:00,  6.51s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num clusters?17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:18<00:00,  6.94s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num clusters?16\n"
     ]
    }
   ],
   "source": [
    "#KMeans clustering\n",
    "train_clusters_post, val_clusters_post, test_clusters_post = clustering(X_train_post,X_val_post,X_test_post,n_clusters=30)\n",
    "train_clusters_pre, val_clusters_pre, test_clusters_pre = clustering(X_train_pre,X_val_pre,X_test_pre,n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cluster dummies\n",
    "train_clusters_dmy = pd.get_dummies(pd.DataFrame({'Cluster':train_clusters}),drop_first=True,prefix='Cluster')\n",
    "val_clustesr_dmy = pd.get_dummies(pd.DataFrame({'Cluster':val_clusters}),drop_first=True,prefix='Cluster')\n",
    "train_clusters_dmy = pd.get_dummies(pd.DataFrame({'Cluster':train_clusters}),drop_first=True,prefix='Cluster')\n",
    "test_clustesr_dmy = pd.get_dummies(pd.DataFrame({'Cluster':test_clusters}),drop_first=True,prefix='Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial naive bayes\n",
    "nb.fit(X_train_post,y_train_le)\n",
    "nb_post_train_preds = nb.predict(X_train_post)\n",
    "nb_post_test_preds = nb.predict(X_test_post)\n",
    "nb_post_val_preds = nb.predict(X_val_post)\n",
    "nb.fit(X_train_pre,y_train_le)\n",
    "nb_pre_train_preds = nb.predict(X_train_pre)\n",
    "nb_pre_test_preds = nb.predict(X_test_pre)\n",
    "nb_pre_val_preds = nb.predict(X_val_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clusters_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NB_pre</th>\n",
       "      <th>NB_post</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Cluster_post</th>\n",
       "      <th>Cluster_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17592</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17594</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17599</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17600</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17601</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17602</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17603</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17604</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17605</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17607</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17610</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17611</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17612</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17613</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17614</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17615</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17617</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17618</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17620</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17621 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NB_pre  NB_post  TFIDF  Cluster_post  Cluster_pre\n",
       "0           1        1      0             3           10\n",
       "1           1        1      2             7            2\n",
       "2           1        1      1            13           11\n",
       "3           0        2      0             8            4\n",
       "4           1        2      1            11           13\n",
       "5           1        1      1             3            3\n",
       "6           1        1      2            11           13\n",
       "7           0        0      0             4            6\n",
       "8           1        1      0             7            2\n",
       "9           1        1      1            15            5\n",
       "10          0        0      0             3            3\n",
       "11          0        0      0             9            9\n",
       "12          1        1      2             5            1\n",
       "13          0        0      0             6            7\n",
       "14          2        2      2             0           12\n",
       "15          1        1      2             3            3\n",
       "16          1        1      1            14           12\n",
       "17          1        1      1             3            3\n",
       "18          1        1      2             9            9\n",
       "19          1        1      1             0            8\n",
       "20          1        1      2             2           14\n",
       "21          0        0      0             3            3\n",
       "22          1        1      1             9           10\n",
       "23          1        1      2             6            7\n",
       "24          1        1      2            10            7\n",
       "25          1        1      2             8            4\n",
       "26          1        1      0             6            7\n",
       "27          0        0      0            11           13\n",
       "28          0        0      1             6            7\n",
       "29          1        1      2            13            0\n",
       "...       ...      ...    ...           ...          ...\n",
       "17591       1        1      2            16           15\n",
       "17592       1        1      1             4            6\n",
       "17593       1        1      2            12            3\n",
       "17594       1        1      0             3            3\n",
       "17595       1        1      0             9            9\n",
       "17596       1        1      1             7            2\n",
       "17597       1        1      0             4            6\n",
       "17598       1        1      2            14            8\n",
       "17599       2        2      0             3            3\n",
       "17600       1        1      1            12           12\n",
       "17601       0        0      1             0           12\n",
       "17602       2        2      1            12            5\n",
       "17603       0        0      0             8            4\n",
       "17604       1        1      0             6            7\n",
       "17605       1        1      1            16           15\n",
       "17606       1        1      1            10            3\n",
       "17607       1        1      0            15            5\n",
       "17608       0        0      2             4            6\n",
       "17609       0        0      0             0           12\n",
       "17610       1        1      0            10           14\n",
       "17611       1        1      0             2           14\n",
       "17612       2        0      2            12           11\n",
       "17613       1        1      2            15            5\n",
       "17614       1        1      1            16           15\n",
       "17615       1        1      2             3            3\n",
       "17616       1        1      0             6            7\n",
       "17617       1        1      0            16           15\n",
       "17618       2        2      2            11           13\n",
       "17619       0        0      0             3            3\n",
       "17620       1        1      1            11           13\n",
       "\n",
       "[17621 rows x 5 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine\n",
    "train_le =pd.DataFrame({'NB_pre':nb_pre_train_preds,'NB_post':nb_post_train_preds,'TFIDF':tfidf_train_preds,\\\n",
    "                        'Cluster_post':train_clusters_post,'Cluster_pre':train_clusters_pre})\n",
    "train_dmy = pd.DataFrame(None,index=train_le.index)\n",
    "for col in train_le.columns:\n",
    "    train_dmy = train_dmy.join(pd.get_dummies(train_le[col].astype(str),drop_first=True,prefix=col) )        \n",
    "val_le =pd.DataFrame({'NB_pre':nb_pre_val_preds,'NB_post':nb_post_val_preds,'TFIDF':tfidf_val_preds,\\\n",
    "                        'Cluster_post':val_clusters_post,'Cluster_pre':val_clusters_pre})\n",
    "val_dmy = pd.DataFrame(None,index=val_le.index)\n",
    "for col in val_le.columns:\n",
    "    val_dmy = val_dmy.join(pd.get_dummies(val_le[col].astype(str),drop_first=True,prefix=col) )\n",
    "test_le =pd.DataFrame({'NB_pre':nb_pre_test_preds,'NB_post':nb_post_test_preds,'TFIDF':tfidf_test_preds,\\\n",
    "                        'Cluster_post':test_clusters_post,'Cluster_pre':test_clusters_pre})\n",
    "test_dmy = pd.DataFrame(None,index=test_le.index)\n",
    "for col in test_le.columns:\n",
    "    test_dmy = test_dmy.join(pd.get_dummies(test_le[col].astype(str),drop_first=True,prefix=col) )                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0764 - accuracy: 0.4090 - val_loss: 1.0514 - val_accuracy: 0.4663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05143, saving model to 255post0.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.0223 - accuracy: 0.5064 - val_loss: 0.9759 - val_accuracy: 0.5613\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05143 to 0.97595, saving model to 255post0.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.9186 - accuracy: 0.5884 - val_loss: 0.9492 - val_accuracy: 0.5271\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.97595 to 0.94919, saving model to 255post0.best.hdf5\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.8122 - accuracy: 0.6630 - val_loss: 0.8709 - val_accuracy: 0.5981\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.94919 to 0.87094, saving model to 255post0.best.hdf5\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7266 - accuracy: 0.7186 - val_loss: 0.7674 - val_accuracy: 0.6818\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87094 to 0.76739, saving model to 255post0.best.hdf5\n",
      "2\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7178 - accuracy: 0.7076 - val_loss: 0.7624 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76236, saving model to 255post1.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6730 - accuracy: 0.7416 - val_loss: 0.6887 - val_accuracy: 0.7150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76236 to 0.68873, saving model to 255post1.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 0.5725 - accuracy: 0.7796 - val_loss: 0.6785 - val_accuracy: 0.7206\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68873 to 0.67850, saving model to 255post1.best.hdf5\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5194 - accuracy: 0.8046 - val_loss: 0.6566 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67850 to 0.65656, saving model to 255post1.best.hdf5\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4604 - accuracy: 0.8298 - val_loss: 0.6412 - val_accuracy: 0.7487\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65656 to 0.64123, saving model to 255post1.best.hdf5\n",
      "3\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5363 - accuracy: 0.7950 - val_loss: 0.6238 - val_accuracy: 0.7492\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62377, saving model to 255post2.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4739 - accuracy: 0.8170 - val_loss: 0.6057 - val_accuracy: 0.7594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62377 to 0.60568, saving model to 255post2.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4338 - accuracy: 0.8396 - val_loss: 0.5852 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60568 to 0.58518, saving model to 255post2.best.hdf5\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3787 - accuracy: 0.8576 - val_loss: 0.6172 - val_accuracy: 0.7589\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58518\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3527 - accuracy: 0.8710 - val_loss: 0.5914 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58518\n",
      "4\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4626 - accuracy: 0.8218 - val_loss: 0.5564 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55644, saving model to 255post3.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4008 - accuracy: 0.8460 - val_loss: 0.5475 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55644 to 0.54747, saving model to 255post3.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3622 - accuracy: 0.8674 - val_loss: 0.5721 - val_accuracy: 0.7640\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54747\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3268 - accuracy: 0.8774 - val_loss: 0.5504 - val_accuracy: 0.7753\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54747\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2968 - accuracy: 0.8944 - val_loss: 0.5633 - val_accuracy: 0.7829\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54747\n",
      "5\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4026 - accuracy: 0.8510 - val_loss: 0.5188 - val_accuracy: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51881, saving model to 255post4.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3572 - accuracy: 0.8634 - val_loss: 0.5166 - val_accuracy: 0.7942\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51881 to 0.51657, saving model to 255post4.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3264 - accuracy: 0.8788 - val_loss: 0.5327 - val_accuracy: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51657\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2906 - accuracy: 0.8954 - val_loss: 0.5287 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51657\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2671 - accuracy: 0.9036 - val_loss: 0.5727 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51657\n",
      "6\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3625 - accuracy: 0.8630 - val_loss: 0.5202 - val_accuracy: 0.8039\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52016, saving model to 255post5.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3167 - accuracy: 0.8818 - val_loss: 0.5117 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52016 to 0.51169, saving model to 255post5.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2852 - accuracy: 0.8906 - val_loss: 0.5225 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51169\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2652 - accuracy: 0.9036 - val_loss: 0.5308 - val_accuracy: 0.8008\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51169\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2422 - accuracy: 0.9170 - val_loss: 0.5568 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51169\n",
      "7\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 0.3225 - accuracy: 0.8832 - val_loss: 0.5228 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52276, saving model to 255post6.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2870 - accuracy: 0.8972 - val_loss: 0.5315 - val_accuracy: 0.8039\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52276\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2495 - accuracy: 0.9128 - val_loss: 0.6012 - val_accuracy: 0.7794\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52276\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2254 - accuracy: 0.9238 - val_loss: 0.5577 - val_accuracy: 0.7993\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52276\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2078 - accuracy: 0.9260 - val_loss: 0.5741 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52276\n",
      "8\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3192 - accuracy: 0.8824 - val_loss: 0.5370 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53701, saving model to 255post7.best.hdf5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2685 - accuracy: 0.9046 - val_loss: 0.5349 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53701 to 0.53488, saving model to 255post7.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2487 - accuracy: 0.9120 - val_loss: 0.5781 - val_accuracy: 0.8059\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53488\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2272 - accuracy: 0.9194 - val_loss: 0.5696 - val_accuracy: 0.8049\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53488\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1904 - accuracy: 0.9354 - val_loss: 0.5955 - val_accuracy: 0.7993\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53488\n",
      "9\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3126 - accuracy: 0.8836 - val_loss: 0.5439 - val_accuracy: 0.8013\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54392, saving model to 255post8.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2619 - accuracy: 0.9066 - val_loss: 0.5444 - val_accuracy: 0.8008\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54392\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2324 - accuracy: 0.9176 - val_loss: 0.5622 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54392\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2202 - accuracy: 0.9208 - val_loss: 0.5924 - val_accuracy: 0.8049\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54392\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1929 - accuracy: 0.9312 - val_loss: 0.5905 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54392\n",
      "10\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2958 - accuracy: 0.8950 - val_loss: 0.5792 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57919, saving model to 255post9.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2432 - accuracy: 0.9102 - val_loss: 0.5578 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57919 to 0.55785, saving model to 255post9.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2162 - accuracy: 0.9204 - val_loss: 0.5808 - val_accuracy: 0.8069\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55785\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1977 - accuracy: 0.9284 - val_loss: 0.5991 - val_accuracy: 0.8069\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55785\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1826 - accuracy: 0.9324 - val_loss: 0.6274 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:26<04:01, 26.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:53<03:33, 26.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [01:19<03:06, 26.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [01:46<02:40, 26.83s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [02:12<02:12, 26.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [02:37<01:43, 25.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [03:04<01:18, 26.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [03:30<00:52, 26.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [03:58<00:26, 26.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [04:23<00:00, 26.35s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "#bagged lstm for post tokens\n",
    "num_words = 5000\n",
    "embed_vec_len = 32\n",
    "max_sequence_len = 255\n",
    "lstm_nn = models.Sequential()\n",
    "lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint('255post.best.hdf5',  verbose=1, save_best_only=True, mode='auto')\n",
    "lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepaths = []\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_train_post)),k=5000)\n",
    "    filepath = '255post'+str(counter)+'.best.hdf5'\n",
    "    filepaths.append(filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "    lstm_nn.fit(X_train_post[sub],y_train_dmy.iloc[sub,:],epochs=5,batch_size=256, validation_data=(X_val_post,y_val_dmy), callbacks=[checkpoint])\n",
    "    counter += 1\n",
    "nn_val_preds = []\n",
    "nn_train_preds = []\n",
    "nn_test_preds = []\n",
    "for filepath in tqdm.tqdm(filepaths):\n",
    "    lstm_nn = models.Sequential()\n",
    "    lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "    lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "    lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "    lstm_nn.load_weights(filepath)\n",
    "    lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    nn_train_preds.append(lstm_nn.predict(X_train_post))\n",
    "    nn_val_preds.append(lstm_nn.predict(X_val_post))\n",
    "    nn_test_preds.append(lstm_nn.predict(X_test_post))\n",
    "nn_train_post_mean = np.asarray(nn_train_preds).mean(axis=0)[:,1:]\n",
    "nn_test_post_mean = np.asarray(nn_test_preds).mean(axis=0)[:,1:]\n",
    "nn_val_post_mean = np.asarray(nn_val_preds).mean(axis=0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0776 - accuracy: 0.4068 - val_loss: 1.0565 - val_accuracy: 0.4152\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05648, saving model to 255pre0.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0294 - accuracy: 0.5214 - val_loss: 0.9960 - val_accuracy: 0.5562\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05648 to 0.99599, saving model to 255pre0.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.9202 - accuracy: 0.6230 - val_loss: 0.8582 - val_accuracy: 0.6435\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99599 to 0.85819, saving model to 255pre0.best.hdf5\n",
      "2\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.8175 - accuracy: 0.6582 - val_loss: 0.7913 - val_accuracy: 0.6762\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79131, saving model to 255pre1.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7124 - accuracy: 0.7100 - val_loss: 0.7140 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79131 to 0.71401, saving model to 255pre1.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6235 - accuracy: 0.7564 - val_loss: 0.6482 - val_accuracy: 0.7278\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71401 to 0.64821, saving model to 255pre1.best.hdf5\n",
      "3\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6222 - accuracy: 0.7556 - val_loss: 0.6226 - val_accuracy: 0.7395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62259, saving model to 255pre2.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5652 - accuracy: 0.7806 - val_loss: 0.6156 - val_accuracy: 0.7569\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62259 to 0.61559, saving model to 255pre2.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.5014 - accuracy: 0.8172 - val_loss: 0.5841 - val_accuracy: 0.7646\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61559 to 0.58412, saving model to 255pre2.best.hdf5\n",
      "4\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5157 - accuracy: 0.7950 - val_loss: 0.5612 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56124, saving model to 255pre3.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4561 - accuracy: 0.8280 - val_loss: 0.5799 - val_accuracy: 0.7671\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56124\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4062 - accuracy: 0.8482 - val_loss: 0.5675 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56124\n",
      "5\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4760 - accuracy: 0.8170 - val_loss: 0.5337 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53367, saving model to 255pre4.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4124 - accuracy: 0.8438 - val_loss: 0.5694 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.53367\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3782 - accuracy: 0.8572 - val_loss: 0.5163 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53367 to 0.51626, saving model to 255pre4.best.hdf5\n",
      "6\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4172 - accuracy: 0.8402 - val_loss: 0.5386 - val_accuracy: 0.7794\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53859, saving model to 255pre5.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3591 - accuracy: 0.8630 - val_loss: 0.5095 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53859 to 0.50953, saving model to 255pre5.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3663 - accuracy: 0.8662 - val_loss: 0.5580 - val_accuracy: 0.7804\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50953\n",
      "7\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3969 - accuracy: 0.8518 - val_loss: 0.5096 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50964, saving model to 255pre6.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3434 - accuracy: 0.8730 - val_loss: 0.5978 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.50964\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3297 - accuracy: 0.8814 - val_loss: 0.5179 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50964\n",
      "8\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3596 - accuracy: 0.8678 - val_loss: 0.5137 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51373, saving model to 255pre7.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3199 - accuracy: 0.8792 - val_loss: 0.5404 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51373\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2971 - accuracy: 0.8888 - val_loss: 0.5268 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51373\n",
      "9\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3197 - accuracy: 0.8790 - val_loss: 0.5110 - val_accuracy: 0.8039\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51103, saving model to 255pre8.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2830 - accuracy: 0.8948 - val_loss: 0.5189 - val_accuracy: 0.8115\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51103\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2778 - accuracy: 0.8984 - val_loss: 0.5374 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51103\n",
      "10\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3290 - accuracy: 0.8772 - val_loss: 0.5100 - val_accuracy: 0.8090\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51001, saving model to 255pre9.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4901 - accuracy: 0.8598 - val_loss: 0.5080 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51001 to 0.50798, saving model to 255pre9.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2759 - accuracy: 0.8994 - val_loss: 0.5109 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:26<04:02, 26.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:53<03:34, 26.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [01:23<03:13, 27.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [01:50<02:45, 27.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [02:16<02:15, 27.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [02:42<01:46, 26.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [03:07<01:18, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [03:34<00:52, 26.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [03:58<00:25, 25.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [04:24<00:00, 25.71s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "#bagged lstm for pre tokens\n",
    "num_words = 5000\n",
    "embed_vec_len = 32\n",
    "max_sequence_len = 255\n",
    "lstm_nn = models.Sequential()\n",
    "lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepaths = []\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_train_pre)),k=5000)\n",
    "    filepath = '255pre'+str(counter)+'.best.hdf5'\n",
    "    filepaths.append(filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "    lstm_nn.fit(X_train_pre[sub],y_train_dmy.iloc[sub,:],epochs=3,batch_size=256, validation_data=(X_val_pre,y_val_dmy), callbacks=[checkpoint])\n",
    "    counter += 1\n",
    "nn_val_preds = []\n",
    "nn_train_preds = []\n",
    "nn_test_preds = []\n",
    "for filepath in tqdm.tqdm(filepaths):\n",
    "    lstm_nn = models.Sequential()\n",
    "    lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "    lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "    lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "    lstm_nn.load_weights(filepath)\n",
    "    lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    nn_train_preds.append(lstm_nn.predict(X_train_pre))\n",
    "    nn_val_preds.append(lstm_nn.predict(X_val_pre))\n",
    "    nn_test_preds.append(lstm_nn.predict(X_test_pre))\n",
    "nn_train_pre_mean = np.asarray(nn_train_preds).mean(axis=0)[:,1:]\n",
    "nn_test_pre_mean = np.asarray(nn_test_preds).mean(axis=0)[:,1:]\n",
    "nn_val_pre_mean = np.asarray(nn_val_preds).mean(axis=0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp_df = train_dmy.join(pd.DataFrame(nn_train_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))\n",
    "val_nlp_df = val_dmy.join(pd.DataFrame(nn_val_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))\n",
    "test_nlp_df = test_dmy.join(pd.DataFrame(nn_test_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_nlp_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-4b1053889f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_nlp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp_df_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_nlp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp_df_val.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_nlp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp_df_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_nlp_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_nlp_df.to_csv('nlp_df_train.csv')\n",
    "val_nlp_df.to_csv('nlp_df_val.csv')\n",
    "test_nlp_df.to_csv('nlp_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dict = ({'LSTM_pre':nn_train_pre_mean,'LSTM_post':nn_train_pre_mean,\\\n",
    "                         'NB_pre':nb_pre_train_preds,'NB_post':nb_post_train_preds,'TFIDF':tfidf_train_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 11s 752us/step - loss: 1.0891 - acc: 0.4010 - val_loss: 1.0855 - val_acc: 0.4137\n",
      "2\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 544us/step - loss: 1.0855 - acc: 0.4031 - val_loss: 1.0885 - val_acc: 0.4137\n",
      "3\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 553us/step - loss: 1.0782 - acc: 0.4071 - val_loss: 1.0936 - val_acc: 0.4040\n",
      "4\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 552us/step - loss: 1.0650 - acc: 0.4269 - val_loss: 1.1047 - val_acc: 0.3687\n",
      "5\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 548us/step - loss: 1.0463 - acc: 0.4547 - val_loss: 1.1145 - val_acc: 0.3580\n",
      "6\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 546us/step - loss: 1.0297 - acc: 0.4701 - val_loss: 1.1315 - val_acc: 0.3672\n",
      "7\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 550us/step - loss: 1.0153 - acc: 0.4858 - val_loss: 1.1445 - val_acc: 0.3504\n",
      "8\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 545us/step - loss: 1.0024 - acc: 0.5001 - val_loss: 1.1688 - val_acc: 0.3315\n",
      "9\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 555us/step - loss: 0.9905 - acc: 0.5072 - val_loss: 1.1722 - val_acc: 0.3544\n",
      "10\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 560us/step - loss: 0.9732 - acc: 0.5198 - val_loss: 1.1859 - val_acc: 0.3468\n",
      "11\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 552us/step - loss: 0.9676 - acc: 0.5251 - val_loss: 1.2096 - val_acc: 0.3412\n",
      "12\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 548us/step - loss: 0.9554 - acc: 0.5369 - val_loss: 1.2103 - val_acc: 0.3463\n",
      "13\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 547us/step - loss: 0.9422 - acc: 0.5471 - val_loss: 1.2244 - val_acc: 0.3478\n",
      "14\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 554us/step - loss: 0.9332 - acc: 0.5467 - val_loss: 1.2267 - val_acc: 0.3590\n",
      "15\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 9s 580us/step - loss: 0.9244 - acc: 0.5535 - val_loss: 1.2784 - val_acc: 0.3769\n",
      "16\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 13s 855us/step - loss: 0.9128 - acc: 0.5692 - val_loss: 1.2475 - val_acc: 0.3718\n",
      "17\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 10s 665us/step - loss: 0.9012 - acc: 0.5737 - val_loss: 1.2735 - val_acc: 0.3698\n",
      "18\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 9s 592us/step - loss: 0.8930 - acc: 0.5803 - val_loss: 1.2663 - val_acc: 0.3621\n",
      "19\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 10s 675us/step - loss: 0.8833 - acc: 0.5829 - val_loss: 1.2929 - val_acc: 0.3514\n",
      "20\n",
      "Train on 15000 samples, validate on 1958 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 9s 622us/step - loss: 0.8722 - acc: 0.5939 - val_loss: 1.3525 - val_acc: 0.3529\n"
     ]
    }
   ],
   "source": [
    "val_loss_50 =[]\n",
    "counter = 0\n",
    "bag_nn_preds_50 = []\n",
    "val_preds_50 = []\n",
    "test_preds_50 = []\n",
    "while counter < 20:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_split_50)),k=15000)\n",
    "    val = [i for i in range(len(X_split_50)) if i not in sub]\n",
    "    history = lstm_nn.fit(X_split_50[sub],y_train_50.iloc[sub,:],epochs=1,batch_size=256, validation_data=(X_val_50,y_val_50))\n",
    "    val_loss_50.append(1-history.history['val_loss'][0])\n",
    "    bag_nn_preds_50.append(lstm_nn.predict(X_train_50))\n",
    "    val_preds_50.append(lstm_nn.predict(X_val_50))\n",
    "    test_preds_50.append(lstm_nn.predict(X_test_50))\n",
    "    counter += 1\n",
    "weights_50 = np.array([i/sum(val_loss_50) for i in val_loss_50])\n",
    "arr_50 = np.zeros(np.array(bag_nn_preds_50).shape)\n",
    "val_arr_50 = np.zeros(np.array(val_preds_50).shape)\n",
    "test_arr_50 = np.zeros(np.array(test_preds_50).shape)\n",
    "for i in range(len(weights)):\n",
    "    arr_50[i]=weights[i]*np.array(bag_nn_preds_50)[i]\n",
    "    val_arr_50[i]=weights[i]*np.array(val_preds_50)[i]\n",
    "    test_arr_50[i]=weights[i]*np.array(test_preds_50)[i]\n",
    "bnp_50_lstm_pre = np.mean(arr_50, axis=0)\n",
    "val_bnp_50_lstm_pre = np.mean(val_arr_50, axis=0)\n",
    "test_bnp_50_lstm_pre = np.mean(test_arr_50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
