{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pronouncing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d35a8d8a449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpronouncing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pronouncing'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from AutoCluster import AutoKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.text\n",
    "y_train = df.author\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, random_state=0,test_size=.1)\n",
    "X_test = test_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate various\n",
    "nb = MultinomialNB()\n",
    "tk = Tokenizer(lower = True,num_words=5000)\n",
    "tfidf = TfidfVectorizer(stop_words='english',max_features=12000)\n",
    "svm = SVC(kernel='linear')\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf with svm\n",
    "X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf.transform(X_test)\n",
    "X_tfidf_val = tfidf.transform(X_val)\n",
    "svm.fit(X_tfidf_train,y_train_le)\n",
    "tfidf_train_preds =svm.predict(X_tfidf_train)\n",
    "tfidf_test_preds =svm.predict(X_tfidf_test)\n",
    "tfidf_val_preds = svm.predict(X_tfidf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text (255 characters, truncating post)\n",
    "tk.fit_on_texts(df.text)\n",
    "train_post = tk.texts_to_sequences(X_train)\n",
    "X_train_post = pad_sequences(train_pre,255,truncating='post')\n",
    "val_post = tk.texts_to_sequences(X_val)\n",
    "X_val_post = pad_sequences(val_pre,255,truncating='post')\n",
    "test_post = tk.texts_to_sequences(test_df.text)\n",
    "X_test_post = pad_sequences(test_pre,255,truncating='post')\n",
    "\n",
    "tk.fit_on_texts(X_train)\n",
    "train_pre = tk.texts_to_sequences(X_train)\n",
    "X_train_pre = pad_sequences(train_pre,255,truncating='pre')\n",
    "val_pre = tk.texts_to_sequences(X_val)\n",
    "X_val_pre = pad_sequences(val_pre,255,truncating='pre')\n",
    "test_pre = tk.texts_to_sequences(X_test)\n",
    "X_test_pre = pad_sequences(test_pre,255,truncating='pre')\n",
    "\n",
    "#reformat y\n",
    "y_train_le = encoder.fit_transform(y_train)\n",
    "y_val_le = encoder.fit_transform(y_val)\n",
    "y_train_dmy = pd.get_dummies(y_train)\n",
    "y_val_dmy = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans clustering\n",
    "train_clusters_post, val_clusters_post, test_clusters_post = AutoKMeans(X_train_post,X_val_post,X_test_post,n_clusters=30)\n",
    "train_clusters_pre, val_clusters_pre, test_clusters_pre = AutoKMeans(X_train_pre,X_val_pre,X_test_pre,n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial naive bayes\n",
    "nb.fit(X_train_post,y_train_le)\n",
    "nb_post_train_preds = nb.predict(X_train_post)\n",
    "nb_post_test_preds = nb.predict(X_test_post)\n",
    "nb_post_val_preds = nb.predict(X_val_post)\n",
    "nb.fit(X_train_pre,y_train_le)\n",
    "nb_pre_train_preds = nb.predict(X_train_pre)\n",
    "nb_pre_test_preds = nb.predict(X_test_pre)\n",
    "nb_pre_val_preds = nb.predict(X_val_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine outputs of models\n",
    "train_le =pd.DataFrame({'NB_pre':nb_pre_train_preds,'NB_post':nb_post_train_preds,'TFIDF':tfidf_train_preds,\\\n",
    "                        'Cluster_post':train_clusters_post,'Cluster_pre':train_clusters_pre})\n",
    "train_dmy = pd.DataFrame(None,index=train_le.index)\n",
    "for col in train_le.columns:\n",
    "    train_dmy = train_dmy.join(pd.get_dummies(train_le[col].astype(str),drop_first=True,prefix=col))        \n",
    "val_le =pd.DataFrame({'NB_pre':nb_pre_val_preds,'NB_post':nb_post_val_preds,'TFIDF':tfidf_val_preds,\\\n",
    "                        'Cluster_post':val_clusters_post,'Cluster_pre':val_clusters_pre})\n",
    "val_dmy = pd.DataFrame(None,index=val_le.index)\n",
    "for col in val_le.columns:\n",
    "    val_dmy = val_dmy.join(pd.get_dummies(val_le[col].astype(str),drop_first=True,prefix=col))\n",
    "test_le =pd.DataFrame({'NB_pre':nb_pre_test_preds,'NB_post':nb_post_test_preds,'TFIDF':tfidf_test_preds,\\\n",
    "                        'Cluster_post':test_clusters_post,'Cluster_pre':test_clusters_pre})\n",
    "test_dmy = pd.DataFrame(None,index=test_le.index)\n",
    "for col in test_le.columns:\n",
    "    test_dmy = test_dmy.join(pd.get_dummies(test_le[col].astype(str),drop_first=True,prefix=col))                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0764 - accuracy: 0.4090 - val_loss: 1.0514 - val_accuracy: 0.4663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05143, saving model to 255post0.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 1.0223 - accuracy: 0.5064 - val_loss: 0.9759 - val_accuracy: 0.5613\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05143 to 0.97595, saving model to 255post0.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.9186 - accuracy: 0.5884 - val_loss: 0.9492 - val_accuracy: 0.5271\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.97595 to 0.94919, saving model to 255post0.best.hdf5\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.8122 - accuracy: 0.6630 - val_loss: 0.8709 - val_accuracy: 0.5981\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.94919 to 0.87094, saving model to 255post0.best.hdf5\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7266 - accuracy: 0.7186 - val_loss: 0.7674 - val_accuracy: 0.6818\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87094 to 0.76739, saving model to 255post0.best.hdf5\n",
      "2\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7178 - accuracy: 0.7076 - val_loss: 0.7624 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76236, saving model to 255post1.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6730 - accuracy: 0.7416 - val_loss: 0.6887 - val_accuracy: 0.7150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76236 to 0.68873, saving model to 255post1.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 0.5725 - accuracy: 0.7796 - val_loss: 0.6785 - val_accuracy: 0.7206\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68873 to 0.67850, saving model to 255post1.best.hdf5\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5194 - accuracy: 0.8046 - val_loss: 0.6566 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67850 to 0.65656, saving model to 255post1.best.hdf5\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4604 - accuracy: 0.8298 - val_loss: 0.6412 - val_accuracy: 0.7487\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65656 to 0.64123, saving model to 255post1.best.hdf5\n",
      "3\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5363 - accuracy: 0.7950 - val_loss: 0.6238 - val_accuracy: 0.7492\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62377, saving model to 255post2.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4739 - accuracy: 0.8170 - val_loss: 0.6057 - val_accuracy: 0.7594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62377 to 0.60568, saving model to 255post2.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4338 - accuracy: 0.8396 - val_loss: 0.5852 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60568 to 0.58518, saving model to 255post2.best.hdf5\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3787 - accuracy: 0.8576 - val_loss: 0.6172 - val_accuracy: 0.7589\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58518\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3527 - accuracy: 0.8710 - val_loss: 0.5914 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58518\n",
      "4\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4626 - accuracy: 0.8218 - val_loss: 0.5564 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55644, saving model to 255post3.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4008 - accuracy: 0.8460 - val_loss: 0.5475 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55644 to 0.54747, saving model to 255post3.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3622 - accuracy: 0.8674 - val_loss: 0.5721 - val_accuracy: 0.7640\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54747\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3268 - accuracy: 0.8774 - val_loss: 0.5504 - val_accuracy: 0.7753\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54747\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2968 - accuracy: 0.8944 - val_loss: 0.5633 - val_accuracy: 0.7829\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54747\n",
      "5\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4026 - accuracy: 0.8510 - val_loss: 0.5188 - val_accuracy: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51881, saving model to 255post4.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3572 - accuracy: 0.8634 - val_loss: 0.5166 - val_accuracy: 0.7942\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51881 to 0.51657, saving model to 255post4.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3264 - accuracy: 0.8788 - val_loss: 0.5327 - val_accuracy: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51657\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2906 - accuracy: 0.8954 - val_loss: 0.5287 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51657\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2671 - accuracy: 0.9036 - val_loss: 0.5727 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51657\n",
      "6\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3625 - accuracy: 0.8630 - val_loss: 0.5202 - val_accuracy: 0.8039\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52016, saving model to 255post5.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3167 - accuracy: 0.8818 - val_loss: 0.5117 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52016 to 0.51169, saving model to 255post5.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2852 - accuracy: 0.8906 - val_loss: 0.5225 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51169\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2652 - accuracy: 0.9036 - val_loss: 0.5308 - val_accuracy: 0.8008\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51169\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2422 - accuracy: 0.9170 - val_loss: 0.5568 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51169\n",
      "7\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 0.3225 - accuracy: 0.8832 - val_loss: 0.5228 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52276, saving model to 255post6.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2870 - accuracy: 0.8972 - val_loss: 0.5315 - val_accuracy: 0.8039\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52276\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2495 - accuracy: 0.9128 - val_loss: 0.6012 - val_accuracy: 0.7794\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52276\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2254 - accuracy: 0.9238 - val_loss: 0.5577 - val_accuracy: 0.7993\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52276\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2078 - accuracy: 0.9260 - val_loss: 0.5741 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52276\n",
      "8\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3192 - accuracy: 0.8824 - val_loss: 0.5370 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53701, saving model to 255post7.best.hdf5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2685 - accuracy: 0.9046 - val_loss: 0.5349 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53701 to 0.53488, saving model to 255post7.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2487 - accuracy: 0.9120 - val_loss: 0.5781 - val_accuracy: 0.8059\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53488\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2272 - accuracy: 0.9194 - val_loss: 0.5696 - val_accuracy: 0.8049\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53488\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1904 - accuracy: 0.9354 - val_loss: 0.5955 - val_accuracy: 0.7993\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53488\n",
      "9\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3126 - accuracy: 0.8836 - val_loss: 0.5439 - val_accuracy: 0.8013\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54392, saving model to 255post8.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2619 - accuracy: 0.9066 - val_loss: 0.5444 - val_accuracy: 0.8008\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54392\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2324 - accuracy: 0.9176 - val_loss: 0.5622 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54392\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2202 - accuracy: 0.9208 - val_loss: 0.5924 - val_accuracy: 0.8049\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54392\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1929 - accuracy: 0.9312 - val_loss: 0.5905 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54392\n",
      "10\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2958 - accuracy: 0.8950 - val_loss: 0.5792 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57919, saving model to 255post9.best.hdf5\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2432 - accuracy: 0.9102 - val_loss: 0.5578 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57919 to 0.55785, saving model to 255post9.best.hdf5\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2162 - accuracy: 0.9204 - val_loss: 0.5808 - val_accuracy: 0.8069\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55785\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1977 - accuracy: 0.9284 - val_loss: 0.5991 - val_accuracy: 0.8069\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55785\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.1826 - accuracy: 0.9324 - val_loss: 0.6274 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:26<04:01, 26.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:53<03:33, 26.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [01:19<03:06, 26.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [01:46<02:40, 26.83s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [02:12<02:12, 26.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [02:37<01:43, 25.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [03:04<01:18, 26.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [03:30<00:52, 26.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [03:58<00:26, 26.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [04:23<00:00, 26.35s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "#bagged lstm for post tokens\n",
    "num_words = 5000\n",
    "embed_vec_len = 32\n",
    "max_sequence_len = 255\n",
    "lstm_nn = models.Sequential()\n",
    "lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint('255post.best.hdf5',  verbose=1, save_best_only=True, mode='auto')\n",
    "lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepaths = []\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_train_post)),k=5000)\n",
    "    filepath = '255post'+str(counter)+'.best.hdf5'\n",
    "    filepaths.append(filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "    lstm_nn.fit(X_train_post[sub],y_train_dmy.iloc[sub,:],epochs=5,batch_size=256, validation_data=(X_val_post,y_val_dmy), callbacks=[checkpoint])\n",
    "    counter += 1\n",
    "nn_val_preds = []\n",
    "nn_train_preds = []\n",
    "nn_test_preds = []\n",
    "for filepath in tqdm.tqdm(filepaths):\n",
    "    lstm_nn = models.Sequential()\n",
    "    lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "    lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "    lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "    lstm_nn.load_weights(filepath)\n",
    "    lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    nn_train_preds.append(lstm_nn.predict(X_train_post))\n",
    "    nn_val_preds.append(lstm_nn.predict(X_val_post))\n",
    "    nn_test_preds.append(lstm_nn.predict(X_test_post))\n",
    "nn_train_post_mean = np.asarray(nn_train_preds).mean(axis=0)[:,1:]\n",
    "nn_test_post_mean = np.asarray(nn_test_preds).mean(axis=0)[:,1:]\n",
    "nn_val_post_mean = np.asarray(nn_val_preds).mean(axis=0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0776 - accuracy: 0.4068 - val_loss: 1.0565 - val_accuracy: 0.4152\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05648, saving model to 255pre0.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 1.0294 - accuracy: 0.5214 - val_loss: 0.9960 - val_accuracy: 0.5562\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05648 to 0.99599, saving model to 255pre0.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.9202 - accuracy: 0.6230 - val_loss: 0.8582 - val_accuracy: 0.6435\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99599 to 0.85819, saving model to 255pre0.best.hdf5\n",
      "2\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.8175 - accuracy: 0.6582 - val_loss: 0.7913 - val_accuracy: 0.6762\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79131, saving model to 255pre1.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7124 - accuracy: 0.7100 - val_loss: 0.7140 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79131 to 0.71401, saving model to 255pre1.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6235 - accuracy: 0.7564 - val_loss: 0.6482 - val_accuracy: 0.7278\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71401 to 0.64821, saving model to 255pre1.best.hdf5\n",
      "3\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.6222 - accuracy: 0.7556 - val_loss: 0.6226 - val_accuracy: 0.7395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62259, saving model to 255pre2.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5652 - accuracy: 0.7806 - val_loss: 0.6156 - val_accuracy: 0.7569\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62259 to 0.61559, saving model to 255pre2.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.5014 - accuracy: 0.8172 - val_loss: 0.5841 - val_accuracy: 0.7646\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61559 to 0.58412, saving model to 255pre2.best.hdf5\n",
      "4\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.5157 - accuracy: 0.7950 - val_loss: 0.5612 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56124, saving model to 255pre3.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4561 - accuracy: 0.8280 - val_loss: 0.5799 - val_accuracy: 0.7671\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56124\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4062 - accuracy: 0.8482 - val_loss: 0.5675 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56124\n",
      "5\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.4760 - accuracy: 0.8170 - val_loss: 0.5337 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53367, saving model to 255pre4.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4124 - accuracy: 0.8438 - val_loss: 0.5694 - val_accuracy: 0.7707\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.53367\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3782 - accuracy: 0.8572 - val_loss: 0.5163 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53367 to 0.51626, saving model to 255pre4.best.hdf5\n",
      "6\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4172 - accuracy: 0.8402 - val_loss: 0.5386 - val_accuracy: 0.7794\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53859, saving model to 255pre5.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3591 - accuracy: 0.8630 - val_loss: 0.5095 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53859 to 0.50953, saving model to 255pre5.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3663 - accuracy: 0.8662 - val_loss: 0.5580 - val_accuracy: 0.7804\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50953\n",
      "7\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3969 - accuracy: 0.8518 - val_loss: 0.5096 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50964, saving model to 255pre6.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3434 - accuracy: 0.8730 - val_loss: 0.5978 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.50964\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3297 - accuracy: 0.8814 - val_loss: 0.5179 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50964\n",
      "8\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3596 - accuracy: 0.8678 - val_loss: 0.5137 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51373, saving model to 255pre7.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.3199 - accuracy: 0.8792 - val_loss: 0.5404 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51373\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.2971 - accuracy: 0.8888 - val_loss: 0.5268 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51373\n",
      "9\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3197 - accuracy: 0.8790 - val_loss: 0.5110 - val_accuracy: 0.8039\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51103, saving model to 255pre8.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2830 - accuracy: 0.8948 - val_loss: 0.5189 - val_accuracy: 0.8115\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51103\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2778 - accuracy: 0.8984 - val_loss: 0.5374 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51103\n",
      "10\n",
      "Train on 5000 samples, validate on 1958 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 0.3290 - accuracy: 0.8772 - val_loss: 0.5100 - val_accuracy: 0.8090\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51001, saving model to 255pre9.best.hdf5\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.4901 - accuracy: 0.8598 - val_loss: 0.5080 - val_accuracy: 0.8034\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51001 to 0.50798, saving model to 255pre9.best.hdf5\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.2759 - accuracy: 0.8994 - val_loss: 0.5109 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:26<04:02, 26.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:53<03:34, 26.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [01:23<03:13, 27.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [01:50<02:45, 27.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [02:16<02:15, 27.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [02:42<01:46, 26.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [03:07<01:18, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [03:34<00:52, 26.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [03:58<00:25, 25.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [04:24<00:00, 25.71s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "#bagged lstm for pre tokens\n",
    "num_words = 5000\n",
    "embed_vec_len = 32\n",
    "max_sequence_len = 255\n",
    "lstm_nn = models.Sequential()\n",
    "lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepaths = []\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "    print(counter+1)\n",
    "    sub = random.sample(range(len(X_train_pre)),k=5000)\n",
    "    filepath = '255pre'+str(counter)+'.best.hdf5'\n",
    "    filepaths.append(filepath)\n",
    "    checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=True, mode='auto')\n",
    "    lstm_nn.fit(X_train_pre[sub],y_train_dmy.iloc[sub,:],epochs=3,batch_size=256, validation_data=(X_val_pre,y_val_dmy), callbacks=[checkpoint])\n",
    "    counter += 1\n",
    "nn_val_preds = []\n",
    "nn_train_preds = []\n",
    "nn_test_preds = []\n",
    "for filepath in tqdm.tqdm(filepaths):\n",
    "    lstm_nn = models.Sequential()\n",
    "    lstm_nn.add(layers.Embedding(num_words, embed_vec_len, input_length=max_sequence_len))\n",
    "    lstm_nn.add(layers.SpatialDropout1D(0.2))\n",
    "    lstm_nn.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm_nn.add(layers.Dense(3, activation='softmax'))\n",
    "    lstm_nn.load_weights(filepath)\n",
    "    lstm_nn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    nn_train_preds.append(lstm_nn.predict(X_train_pre))\n",
    "    nn_val_preds.append(lstm_nn.predict(X_val_pre))\n",
    "    nn_test_preds.append(lstm_nn.predict(X_test_pre))\n",
    "nn_train_pre_mean = np.asarray(nn_train_preds).mean(axis=0)[:,1:]\n",
    "nn_test_pre_mean = np.asarray(nn_test_preds).mean(axis=0)[:,1:]\n",
    "nn_val_pre_mean = np.asarray(nn_val_preds).mean(axis=0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_df_train = train_dmy.join(pd.DataFrame(nn_train_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))\n",
    "nlp_df_val.csv = val_dmy.join(pd.DataFrame(nn_val_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))\n",
    "test_nlp_df = test_dmy.join(pd.DataFrame(nn_test_pre_mean,columns=('LSTM_pre_1','LSTM_pre_2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp_df.to_csv('nlp_df_train.csv')\n",
    "val_nlp_df.to_csv('nlp_df_val.csv')\n",
    "test_nlp_df.to_csv('nlp_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
